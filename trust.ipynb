{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.2.2\n",
      "polars version: 1.24.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "print(f'numpy version: {np.__version__}')\n",
    "print(f'polars version: {pl.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1 # random seed to ensure reproducibility\n",
    "DATA_LOCATION = 'Mimic3_Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2852933, 3)\n"
     ]
    }
   ],
   "source": [
    "df_chartevents = pl.read_parquet(f'{DATA_LOCATION}/chartevents_trust.parquet')\n",
    "print(df_chartevents.shape)\n",
    "#print(df_chartevents.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2082294, 6)\n"
     ]
    }
   ],
   "source": [
    "df_noteevents = pl.read_parquet(f'{DATA_LOCATION}/noteevents_trust.parquet')\n",
    "print(df_noteevents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patients labeled as trustful: 53915\n",
      "patients labeled as mistrustful: 611\n",
      "num_features: 620\n",
      "{'mistrust': 1, 'trust': 0}\n"
     ]
    }
   ],
   "source": [
    "chartevents_features = {}\n",
    "grouped = df_chartevents.group_by('HADM_ID')\n",
    "for hadm_id, rows in grouped:\n",
    "    feats = {}\n",
    "    for row in rows.iter_rows():\n",
    "        label = row[1].lower()\n",
    "        row_value = row[2]\n",
    "        if row_value is None:\n",
    "            val = 'none'\n",
    "        else:\n",
    "            val = row_value.lower()\n",
    "        \n",
    "        if 'reason for restraint' in label:       \n",
    "            \n",
    "            if (val == 'not applicable') or (val == 'none'):\n",
    "                val = 'none'\n",
    "            elif ('threat' in val) or ('acute risk of' in val):\n",
    "                val = 'threat of harm'\n",
    "            elif ('confusion' in val) or ('delirium' in val) or (val == 'impaired judgment') or (val == 'sundowning'):\n",
    "                val = 'confusion/delirium'\n",
    "            elif ('occurence' in val) or (val == 'severe physical agitation') or (val == 'violent/self des'):\n",
    "                val = 'prescence of violence'\n",
    "            elif (val == 'ext/txinterfere') or (val == 'protection of lines and tubes') or (val == 'treatment interference'):\n",
    "                val = 'treatment interference'\n",
    "            elif 'risk for fall' in val:\n",
    "                val = 'risk for falls'\n",
    "            else:\n",
    "                val = val\n",
    "            \n",
    "            feats[('reason for restraint', val)] = 1\n",
    "\n",
    "        elif 'restraint location' in label:\n",
    "                \n",
    "            if val == 'none':\n",
    "                val = 'none'\n",
    "            elif '4 point rest' in val:\n",
    "                val = '4 point restraint'\n",
    "            else:\n",
    "                val = 'some restraint'\n",
    "            \n",
    "            feats[('restraint location', val)] = 1\n",
    "\n",
    "        elif 'restraint device' in label:\n",
    "                \n",
    "            if 'sitter' in val:\n",
    "                val = 'sitter'\n",
    "            elif 'limb' in val:\n",
    "                val = 'limb'\n",
    "            else:\n",
    "                val = val\n",
    "            \n",
    "            feats[('restraint device', val)] = 1\n",
    "            \n",
    "        elif 'bath' in label:\n",
    "            if 'part' in label:\n",
    "                val = 'partial'\n",
    "            elif 'self' in val:\n",
    "                val = 'self'\n",
    "            elif 'refused' in val:\n",
    "                val = 'refused'\n",
    "            elif 'shave' in val:\n",
    "                val = 'shave'\n",
    "            elif 'hair' in val:\n",
    "                val = 'hair'\n",
    "            elif 'none' in val:\n",
    "                val = 'none'\n",
    "            else:\n",
    "                val = 'done'\n",
    "            \n",
    "            feats[('bath', val)] = 1\n",
    "\n",
    "        elif label in ['behavior', 'behavioral state']:\n",
    "            #feats[('behavior', val)] = 1\n",
    "            pass\n",
    "            \n",
    "        elif label.startswith('pain level'):\n",
    "            feats[('pain level', val)] = 1\n",
    "            \n",
    "        elif label.startswith('pain management'):\n",
    "            #feats[('pain management', val)] = 1\n",
    "            pass\n",
    "        elif label.startswith('pain type'):\n",
    "            #feats[('pain type', val)] = 1\n",
    "            pass\n",
    "        elif label.startswith('pain cause'):\n",
    "            #feats[('pain cause', val)] = 1\n",
    "            pass\n",
    "        elif label.startswith('pain location'):\n",
    "            #feats[('pain location', val)] = 1\n",
    "            pass\n",
    "            \n",
    "        elif label.startswith('education topic'):\n",
    "            feats[('education topic', val)] = 1\n",
    "\n",
    "        elif label.startswith('safety measures'):\n",
    "            feats[('safety measures', val)] = 1\n",
    "            \n",
    "        elif label.startswith('side rails'):\n",
    "            feats[('side rails', val)] = 1\n",
    "    \n",
    "        elif label.startswith('status and comfort'):\n",
    "            feats[('status and comfort', val)] = 1\n",
    "\n",
    "        elif 'informed' in label:\n",
    "            feats[('informed', val)] = 1        \n",
    "        else:\n",
    "\n",
    "            if type(row_value) == type(''):\n",
    "                # extract phrase\n",
    "                featname = (label, row_value.lower())\n",
    "                value = 1.0\n",
    "                feats[featname] = value\n",
    "            elif row_value is None:\n",
    "                featname = (label,'none')\n",
    "                value = 1.0\n",
    "                feats[featname] = value\n",
    "            else: \n",
    "                featname = (label,)\n",
    "                value = value\n",
    "                feats[featname] = value\n",
    "                pass\n",
    "    \n",
    "    chartevents_features[hadm_id] = feats\n",
    "\n",
    "all_hadm_ids = set(df_chartevents['HADM_ID'].unique())\n",
    "mistrust_ids = set([])\n",
    "count = 0\n",
    "for hadm_id, rows in df_noteevents.group_by('HADM_ID'):\n",
    "    # This is customizable to various note-based definitions of what to look for\n",
    "    mistrust = False\n",
    "    for row in rows.iter_rows():\n",
    "        text = row[2].lower()\n",
    "        if 'noncompliance' in text:\n",
    "            mistrust = True\n",
    "    # add the ID\n",
    "    if mistrust:\n",
    "        if isinstance(hadm_id, tuple):\n",
    "            if len(hadm_id) > 1:\n",
    "                raise ValueError('hadm_id is weird tuple', hadm_id)\n",
    "            hadm_id = hadm_id[0]\n",
    "        if hadm_id is not None:\n",
    "            mistrust_ids.add(int(hadm_id))\n",
    "\n",
    "# binary labels\n",
    "trust_labels_noncompliance = {hadm_id:'trust' for hadm_id in all_hadm_ids}\n",
    "for hadm_id in mistrust_ids:\n",
    "    trust_labels_noncompliance[hadm_id] = 'mistrust'\n",
    "    \n",
    "print('patients labeled as trustful:', len([y for y in trust_labels_noncompliance.values() if y=='trust']))\n",
    "print('patients labeled as mistrustful:', len([y for y in trust_labels_noncompliance.values() if y=='mistrust']))\n",
    "\n",
    "vect = DictVectorizer()\n",
    "vect.fit(chartevents_features.values())\n",
    "print('num_features:', len(vect.get_feature_names_out()))\n",
    "\n",
    "trust_Y_vect = {'mistrust': 1, 'trust': 0}\n",
    "print(trust_Y_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patients labeled as trustful: 739\n",
      "patients labeled as mistrustful: 270\n"
     ]
    }
   ],
   "source": [
    "autopsy_consent = []\n",
    "autopsy_decline = []\n",
    "\n",
    "grouped = df_noteevents.group_by('HADM_ID')\n",
    "for hadm_id, rows in grouped:\n",
    "    if isinstance(hadm_id, tuple):\n",
    "        if len(hadm_id) > 1:\n",
    "            raise ValueError('hadm_id is weird tuple', hadm_id)\n",
    "        hadm_id = hadm_id[0]\n",
    "    if hadm_id is not None:\n",
    "        mistrust_ids.add(int(hadm_id))\n",
    "    consented = False\n",
    "    declined = False\n",
    "    for row in rows.iter_rows():\n",
    "        text = row[2].lower()\n",
    "        for line in text.lower().split('\\n'):\n",
    "            if 'autopsy' in line:\n",
    "                if 'decline' in line:\n",
    "                    declined = True\n",
    "                if 'not consent' in line:\n",
    "                    declined = True\n",
    "                if 'refuse' in line:\n",
    "                    declined = True\n",
    "                if 'denied' in line:\n",
    "                    declined = True\n",
    "                if 'consent' in line:\n",
    "                    consented = True\n",
    "                if 'agree' in line:\n",
    "                    consented = True\n",
    "                if 'request' in line:\n",
    "                    consented = True\n",
    "                \n",
    "    # probably some \"declined donation but consented to autopsy\" or something confusing. just ignore hard cases\n",
    "    if consented and declined:\n",
    "        continue\n",
    "\n",
    "    if consented:\n",
    "        autopsy_consent.append(hadm_id)\n",
    "    if declined:\n",
    "        autopsy_decline.append(hadm_id)\n",
    "        \n",
    "# binary labels\n",
    "trust_labels_autopsy = {}\n",
    "for hadm_id in autopsy_consent:\n",
    "    trust_labels_autopsy[int(hadm_id)] = 'mistrust'\n",
    "for hadm_id in autopsy_decline:\n",
    "    trust_labels_autopsy[int(hadm_id)] = 'trust'\n",
    "\n",
    "print('patients labeled as trustful:', len([y for y in trust_labels_autopsy.values() if y=='trust']))\n",
    "print('patients labeled as mistrustful:', len([y for y in trust_labels_autopsy.values() if y=='mistrust']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents_features = {k[0]: v for k, v in chartevents_features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(ids, ratio=0.7, seed=SEED):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(ids)\n",
    "    train = ids[:int(len(ids)*ratio)]\n",
    "    test  = ids[int(len(ids)*ratio):]\n",
    "    return train, test\n",
    "\n",
    "def compute_stats_binary(task, pred, probas, ref, labels_map):\n",
    "    \"\"\"\n",
    "    Core function for calculating binary classification metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        task (str): Task description/identifier\n",
    "        pred (np.ndarray): Predicted labels (0 or 1)\n",
    "        probas (np.ndarray): Model confidence scores for positive class\n",
    "        ref (np.ndarray): Ground truth labels\n",
    "        labels_map (dict): Mapping of label indices to class names\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing accuracy, precision, recall, f1, \n",
    "              auc, sensitivity, and specificity\n",
    "    \"\"\"\n",
    "    # Validate predictions match probability thresholds\n",
    "    assert np.array_equal(pred, (probas > 0).astype(int)), \"Predictions mismatch with probability thresholds\"\n",
    "    \n",
    "    # Calculate confusion matrix using sklearn\n",
    "    conf = confusion_matrix(ref, pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    \n",
    "    # Compute metrics using sklearn functions\n",
    "    precision = precision_score(ref, pred, zero_division=0)\n",
    "    recall = recall_score(ref, pred, zero_division=0)\n",
    "    f1 = f1_score(ref, pred, zero_division=0)\n",
    "    accuracy = accuracy_score(ref, pred)\n",
    "    specificity = tn / (tn + fp + 1e-9)\n",
    "    \n",
    "    # Calculate AUC if applicable\n",
    "    auc = roc_auc_score(ref, probas) if len(np.unique(ref)) == 2 else None\n",
    "    \n",
    "    # Print formatted results\n",
    "    print(f\"\\nTask: {task}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf}\")\n",
    "    print(f\"Specificity:\\t{specificity:.3f}\")\n",
    "    print(f\"Sensitivity:\\t{recall:.3f}\")  # Sensitivity = Recall\n",
    "    if auc is not None:\n",
    "        print(f\"AUC:\\t\\t{auc:.3f}\")\n",
    "    print(f\"Accuracy:\\t{accuracy:.3f}\")\n",
    "    print(f\"Precision:\\t{precision:.3f}\")\n",
    "    print(f\"F1 Score:\\t{f1:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'sensitivity': recall,\n",
    "        'specificity': specificity\n",
    "    }\n",
    "\n",
    "def classification_results(svm, labels_map, X, Y, task):\n",
    "    \"\"\"\n",
    "    Generates classification results from an SVM model.\n",
    "    \n",
    "    Parameters:\n",
    "        svm: Trained SVM classifier\n",
    "        labels_map (dict): Label index to name mapping\n",
    "        X (np.ndarray): Feature matrix\n",
    "        Y (np.ndarray): True labels\n",
    "        task (str): Task description/identifier\n",
    "        \n",
    "    Returns:\n",
    "        dict: Classification metrics\n",
    "    \"\"\"\n",
    "    decision_scores = svm.decision_function(X)\n",
    "    \n",
    "    # Handle binary classification formatting\n",
    "    if len(labels_map) == 2:\n",
    "        probas = np.column_stack([-decision_scores, decision_scores])\n",
    "    else:\n",
    "        probas = decision_scores\n",
    "        \n",
    "    pred = probas.argmax(axis=1)\n",
    "    return compute_stats_binary(task, pred, probas[:, 1], Y, labels_map)\n",
    "\n",
    "# display informative features\n",
    "\n",
    "def classification_analyze(task: str, vect, clf, labels_map: dict, num_feats=10):\n",
    "    \"\"\"\n",
    "    Print the most positive and negative features for a binary linear classifier.\n",
    "\n",
    "    Parameters:\n",
    "        task (str): Name or description of the classification task.\n",
    "        vect: Fitted vectorizer with a `vocabulary_` attribute.\n",
    "        clf: Trained linear classifier (must have `coef_` attribute).\n",
    "        labels_map (dict): Mapping from label names to integer indices.\n",
    "        num_feats (int): Number of top features to display for each class.\n",
    "    \"\"\"\n",
    "    # Map feature indices back to feature names\n",
    "    ind2feat = {i: str(f) for f, i in vect.vocabulary_.items()}\n",
    "\n",
    "    # Get sorted label names by their index\n",
    "    sorted_labels = [label for label, i in sorted(labels_map.items(), key=lambda t: t[1])]\n",
    "    if len(sorted_labels) != 2:\n",
    "        raise ValueError(\"This function is for binary classification only.\")\n",
    "\n",
    "    coef = clf.coef_[0]\n",
    "    sorted_indices = np.argsort(coef)\n",
    "\n",
    "    # Most negative features (class 0)\n",
    "    neg_indices = sorted_indices[:num_feats]\n",
    "    # Most positive features (class 1)\n",
    "    pos_indices = sorted_indices[-num_feats:][::-1]\n",
    "\n",
    "    print(f\"\\nTop {num_feats} features for class '{sorted_labels[1]}' (positive):\")\n",
    "    for idx in pos_indices:\n",
    "        val = coef[idx]\n",
    "        word = ind2feat.get(idx, f\"<UNK-{idx}>\")\n",
    "        if val > 1e-4:\n",
    "            print(f\"  {word:<25}: {val:7.4f}\")\n",
    "\n",
    "    print(f\"\\nTop {num_feats} features for class '{sorted_labels[0]}' (negative):\")\n",
    "    for idx in neg_indices:\n",
    "        val = coef[idx]\n",
    "        word = ind2feat.get(idx, f\"<UNK-{idx}>\")\n",
    "        if val < -1e-4:\n",
    "            print(f\"  {word:<25}: {val:7.4f}\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patients: 54510\n",
      "LogisticRegression(C=0.1, random_state=1, tol=0.01)\n",
      "\n",
      "Task: train: noncompliance\n",
      "Confusion Matrix:\n",
      "[[37738     0]\n",
      " [  419     0]]\n",
      "Specificity:\t1.000\n",
      "Sensitivity:\t0.000\n",
      "AUC:\t\t0.554\n",
      "Accuracy:\t0.989\n",
      "Precision:\t0.000\n",
      "F1 Score:\t0.000\n",
      "\n",
      "Task: test: noncompliance\n",
      "Confusion Matrix:\n",
      "[[16177     0]\n",
      " [  176     0]]\n",
      "Specificity:\t1.000\n",
      "Sensitivity:\t0.000\n",
      "AUC:\t\t0.508\n",
      "Accuracy:\t0.989\n",
      "Precision:\t0.000\n",
      "F1 Score:\t0.000\n",
      "\n",
      "Top 15 features for class 'mistrust' (positive):\n",
      "  ('support systems', 'parents'):  0.0169\n",
      "  ('reason for restraint', 'confusion/delirium'):  0.0122\n",
      "  ('non-violent restraints', 'off'):  0.0119\n",
      "  ('social work consult', '1'):  0.0106\n",
      "  ('judgement', 'impaired'):  0.0104\n",
      "  ('education topic', 'diabetic'):  0.0099\n",
      "  ('safety measures', '1:1 time with patient'):  0.0097\n",
      "  ('side rails', 'all rails up - specialty beds only'):  0.0092\n",
      "  ('restraint device', '4 side rails'):  0.0082\n",
      "  ('restraints evaluated_v2', 'continued'):  0.0080\n",
      "  ('reason for restraint', 'risk for falls'):  0.0075\n",
      "  ('side rails', 'all rails up (restraint)'):  0.0068\n",
      "  ('restraint ordered (non-violent)', 'not done'):  0.0066\n",
      "  ('orientation', 'oriented x2'):  0.0066\n",
      "  ('status and comfort', 'agitated'):  0.0064\n",
      "\n",
      "Top 15 features for class 'trust' (negative):\n",
      "  ('state', 'alert')       : -0.5258\n",
      "  ('pain', 'none')         : -0.4803\n",
      "  ('stress', 'none')       : -0.4717\n",
      "  ('pain present', 'no')   : -0.3803\n",
      "  ('family communication', 'family visited'): -0.3358\n",
      "  ('verbal response', '5 oriented'): -0.3295\n",
      "  ('orientation', 'oriented x 3'): -0.3029\n",
      "  ('support systems', 'spouse'): -0.2620\n",
      "  ('pain present', 'yes')  : -0.2547\n",
      "  ('side rails', 'side rails up'): -0.2508\n",
      "  ('restraint location', 'none'): -0.2349\n",
      "  ('riker-sas scale', 'calm/cooperative'): -0.2243\n",
      "  ('support systems', 'children'): -0.2157\n",
      "  ('skin care', 'done')    : -0.1991\n",
      "  ('pain assess method', 'pt verbalized'): -0.1969\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CLASSIFIER: noncompliance\n",
    "\n",
    "noncompliance_cohort = list(set(trust_labels_noncompliance.keys()) & all_hadm_ids)\n",
    "print('patients:', len(noncompliance_cohort))\n",
    "\n",
    "# train/test split\n",
    "noncompliance_train_ids, noncompliance_test_ids = data_split(noncompliance_cohort)\n",
    "\n",
    "# select pre-computed features\n",
    "noncompliance_train_features = [chartevents_features[hadm_id] for hadm_id in noncompliance_train_ids]\n",
    "noncompliance_test_features  = [chartevents_features[hadm_id] for hadm_id in noncompliance_test_ids ]\n",
    "\n",
    "# vectorize features\n",
    "noncompliance_train_X = vect.transform(noncompliance_train_features)\n",
    "noncompliance_test_X  = vect.transform(noncompliance_test_features)\n",
    "\n",
    "# select labels\n",
    "noncompliance_train_Y = [trust_Y_vect[trust_labels_noncompliance[hadm_id]] for hadm_id in noncompliance_train_ids]\n",
    "noncompliance_test_Y = [trust_Y_vect[trust_labels_noncompliance[hadm_id]] for hadm_id in noncompliance_test_ids]\n",
    "\n",
    "# fit model\n",
    "noncompliance_svm = LogisticRegression(C=0.1, penalty='l2', tol=0.01, random_state=SEED)\n",
    "noncompliance_svm.fit(noncompliance_train_X, noncompliance_train_Y)\n",
    "print(noncompliance_svm)\n",
    "\n",
    "# evaluate model\n",
    "classification_results(noncompliance_svm, trust_Y_vect, noncompliance_train_X, noncompliance_train_Y, 'train: noncompliance')\n",
    "classification_results(noncompliance_svm, trust_Y_vect,  noncompliance_test_X,  noncompliance_test_Y, 'test: noncompliance')\n",
    "\n",
    "# most informative features\n",
    "classification_analyze('noncompliance', vect, noncompliance_svm, trust_Y_vect, num_feats=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patients: 997\n",
      "LogisticRegression(C=0.1, random_state=1, tol=0.01)\n",
      "\n",
      "Task: train: autopsy\n",
      "Confusion Matrix:\n",
      "[[498  12]\n",
      " [138  49]]\n",
      "Specificity:\t0.976\n",
      "Sensitivity:\t0.262\n",
      "AUC:\t\t0.850\n",
      "Accuracy:\t0.785\n",
      "Precision:\t0.803\n",
      "F1 Score:\t0.395\n",
      "\n",
      "Task: test:  autopsy\n",
      "Confusion Matrix:\n",
      "[[204  17]\n",
      " [ 73   6]]\n",
      "Specificity:\t0.923\n",
      "Sensitivity:\t0.076\n",
      "AUC:\t\t0.550\n",
      "Accuracy:\t0.700\n",
      "Precision:\t0.261\n",
      "F1 Score:\t0.118\n",
      "\n",
      "Top 15 features for class 'mistrust' (positive):\n",
      "  ('pain level', 'unable to score'):  0.3888\n",
      "  ('pain level', 'moderate'):  0.3389\n",
      "  ('family communication', 'family called'):  0.3035\n",
      "  ('restraint type', 'soft limb'):  0.2744\n",
      "  ('riker-sas scale', 'very sedated'):  0.2585\n",
      "  ('pain level', 'moderate to severe'):  0.2441\n",
      "  ('verbal response', '1.0 et/trach'):  0.2343\n",
      "  ('education barrier', 'medicated'):  0.2331\n",
      "  ('reason for restraint', 'treatment interference'):  0.2314\n",
      "  ('pain present', 'yes')  :  0.2312\n",
      "  ('skin care', 'done')    :  0.2181\n",
      "  ('orientation', 'oriented x 3'):  0.2180\n",
      "  ('restraints evaluated', 'restraintreapply'):  0.2064\n",
      "  ('pain level', 'severe') :  0.1999\n",
      "  ('family meeting', '1')  :  0.1956\n",
      "\n",
      "Top 15 features for class 'trust' (negative):\n",
      "  ('support systems', 'children'): -0.4803\n",
      "  ('education topic', 'coping'): -0.4280\n",
      "  ('pain present', 'no')   : -0.4267\n",
      "  ('pain level', 'none to mild'): -0.3346\n",
      "  ('family communication', 'family conferenc'): -0.3324\n",
      "  ('informed', 'yes')      : -0.3180\n",
      "  ('riker-sas scale', 'none'): -0.3175\n",
      "  ('follows commands', 'none'): -0.2963\n",
      "  ('family communication', 'fam talked to md'): -0.2901\n",
      "  ('safety measures', 'medication reviewed'): -0.2847\n",
      "  ('riker-sas scale', 'agitated'): -0.2645\n",
      "  ('orientation', 'oriented x 1'): -0.2640\n",
      "  ('education topic', 'icu environment'): -0.2530\n",
      "  ('pain level', '2-mild') : -0.2378\n",
      "  ('pain assessment method', 'patient verbalized'): -0.2204\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CLASSIFIER: autopsy\n",
    "\n",
    "autopsy_cohort = list(set(trust_labels_autopsy.keys()) & set(df_chartevents['HADM_ID'].unique()))\n",
    "print('patients:', len(autopsy_cohort))\n",
    "\n",
    "# train/test split\n",
    "autopsy_train_ids, autopsy_test_ids = data_split(autopsy_cohort)\n",
    "\n",
    "# select pre-computed features\n",
    "autopsy_train_features = [chartevents_features[hadm_id] for hadm_id in autopsy_train_ids]\n",
    "autopsy_test_features  = [chartevents_features[hadm_id] for hadm_id in autopsy_test_ids ]\n",
    "\n",
    "# vectorize features\n",
    "autopsy_train_X = vect.transform(autopsy_train_features)\n",
    "autopsy_test_X  = vect.transform(autopsy_test_features)\n",
    "\n",
    "# select labels\n",
    "autopsy_train_Y = [trust_Y_vect[trust_labels_autopsy[hadm_id]] for hadm_id in autopsy_train_ids]\n",
    "autopsy_test_Y  = [trust_Y_vect[trust_labels_autopsy[hadm_id]] for hadm_id in autopsy_test_ids ]\n",
    "\n",
    "# fit model\n",
    "autopsy_svm = LogisticRegression(C=0.1, penalty='l2', tol=0.01, random_state=SEED)\n",
    "autopsy_svm.fit(autopsy_train_X, autopsy_train_Y)\n",
    "print(autopsy_svm)\n",
    "\n",
    "# evaluate model\n",
    "classification_results(autopsy_svm, trust_Y_vect, autopsy_train_X, autopsy_train_Y, 'train: autopsy')\n",
    "classification_results(autopsy_svm, trust_Y_vect,  autopsy_test_X,  autopsy_test_Y, 'test:  autopsy')\n",
    "\n",
    "# most informative features\n",
    "classification_analyze('trust', vect, autopsy_svm, trust_Y_vect, num_feats=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
