{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, sys, os\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, precision_score, accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1 # random seed to ensure reproducibility\n",
    "DATA_LOCATION = 'Mimic3_Data'\n",
    "LOG_LOCATION = 'ModelResults'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>VALUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150008</td>\n",
       "      <td>Code Status</td>\n",
       "      <td>Full Code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120815</td>\n",
       "      <td>Code Status</td>\n",
       "      <td>Full Code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194687</td>\n",
       "      <td>Code Status</td>\n",
       "      <td>Comfort Measures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151808</td>\n",
       "      <td>Code Status</td>\n",
       "      <td>Comfort Measures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>185444</td>\n",
       "      <td>Code Status</td>\n",
       "      <td>Full Code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID        LABEL             VALUE\n",
       "0   150008  Code Status         Full Code\n",
       "1   120815  Code Status         Full Code\n",
       "2   194687  Code Status  Comfort Measures\n",
       "3   151808  Code Status  Comfort Measures\n",
       "4   185444  Code Status         Full Code"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_status_df = pd.read_parquet(f'{DATA_LOCATION}/code_status.parquet')\n",
    "code_status_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{None, 'Comfort measures only', 'Comfort Measures', 'DNR / DNI', 'Do Not Resuscita', 'Full Code', 'Full code', 'DNR (do not resuscitate)', 'Do Not Intubate', 'Other/Remarks', 'DNI (do not intubate)', 'CPR Not Indicate'}\n"
     ]
    }
   ],
   "source": [
    "code_labels = {}\n",
    "for i,row in code_status_df.iterrows():\n",
    "    if row['VALUE'] is not None:\n",
    "        if ('DNR' in row['VALUE']) or ('DNI' in row['VALUE']) or ('Comfort' in row['VALUE']) or ('Do Not' in row['VALUE']):\n",
    "            label = 'DNR/CMO'\n",
    "        elif (row['VALUE'] == 'Full Code') or (row['VALUE'] == 'Full code'):\n",
    "            label = 'Full Code'\n",
    "    code_labels[row['HADM_ID']] = label\n",
    "print(set(code_status_df['VALUE'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_race(race):\n",
    "    if 'HISPANIC' in race:\n",
    "        return 'Hispanic'\n",
    "    if 'SOUTH AMERICAN' in race:\n",
    "        return 'Hispanic'\n",
    "    if 'AMERICAN INDIAN' in race:\n",
    "        return 'Native American'\n",
    "    if 'ASIAN' in race:\n",
    "        return 'Asian'\n",
    "    if 'BLACK' in race:\n",
    "        return 'Black'\n",
    "    if 'WHITE' in race:\n",
    "        return 'White'\n",
    "    return 'Other'\n",
    "\n",
    "def normalize_insurance(insurance):\n",
    "    if insurance in ['Medicare', 'Medicaid', 'Government']:\n",
    "        return 'Public'\n",
    "    else:\n",
    "        return insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_df = pd.read_parquet(f'{DATA_LOCATION}/discharge.parquet')\n",
    "discharge_df = discharge_df[['HADM_ID', 'DISCHARGE_LOCATION']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ama_labels = {}\n",
    "for i,row in discharge_df.iterrows():\n",
    "    if row['DISCHARGE_LOCATION'] == 'LEFT AGAINST MEDICAL ADVI':\n",
    "        label = 'AMA'\n",
    "    else:\n",
    "        label = 'compliant'\n",
    "    ama_labels[row['HADM_ID']] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality = pd.read_parquet(f'{DATA_LOCATION}/mortality.parquet')\n",
    "\n",
    "# binary labels\n",
    "mortality_labels = {}\n",
    "for i,row in mortality.iterrows():\n",
    "    if row['HOSPITAL_EXPIRE_FLAG']:\n",
    "        label = 'deceased'\n",
    "    else:\n",
    "        label = 'survived'\n",
    "    mortality_labels[row['HADM_ID']] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all prediction targets into one DataFrame\n",
    "all_labels = pd.merge(\n",
    "    pd.merge(\n",
    "        pd.DataFrame(ama_labels.items(), columns=['HADM_ID', 'AMA']),\n",
    "        pd.DataFrame(code_labels.items(), columns=['HADM_ID', 'CS']),\n",
    "        on='HADM_ID',\n",
    "        how='outer'\n",
    "    ),\n",
    "    pd.DataFrame(mortality_labels.items(), columns=['HADM_ID', 'MORTALITY']),\n",
    "    on='HADM_ID',\n",
    "    how='outer'\n",
    ").set_index('HADM_ID')\n",
    "all_labels['AMA'] = np.where(all_labels['AMA'] == 'AMA', 1, 0)\n",
    "all_labels['CS'] = np.where(all_labels['CS'] == 'DNR/CMO', 1, 0)\n",
    "all_labels['MORTALITY'] = np.where(all_labels['MORTALITY'] == 'deceased', 1, 0)\n",
    "#all_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(ids, ratio=0.7, seed=SEED):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(ids)\n",
    "    train = ids[:int(len(ids)*ratio)]\n",
    "    test  = ids[int(len(ids)*ratio):]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(task, vect, clf, count_top=False):\n",
    "\n",
    "    ind2feat =  {i:f for f,i in vect.vocabulary_.items()}\n",
    "\n",
    "    # create a 2-by-m matrix for biary, rather than relying on 1-p bullshit\n",
    "    coef_ = clf.coef_\n",
    "    \n",
    "    # most informative features\n",
    "    print(task)\n",
    "    informative_feats = np.argsort(coef_)\n",
    "    \n",
    "    if len(informative_feats.shape) == 2:\n",
    "        informative_feats = informative_feats[0,:]\n",
    "        coef_ = coef_[0,:]\n",
    "        \n",
    "    # display what each feature is\n",
    "    for feat in reversed(informative_feats):\n",
    "        val = coef_[feat]\n",
    "\n",
    "        word = ind2feat[feat]\n",
    "        print('\\t%-25s: %7.4f' % (word,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(pred, P, ref, labels_map):\n",
    "    if len(labels_map) == 2:\n",
    "        scores = P[:,1] - P[:,0]\n",
    "        res = compute_stats_binary(pred, scores, ref, labels_map)\n",
    "    else:\n",
    "        res = compute_stats_multiclass(pred, P, ref, labels_map)\n",
    "    return res\n",
    "\n",
    "def compute_stats_binary(pred, probas, ref, labels_map):\n",
    "    \"\"\"\n",
    "    Core function for calculating binary classification metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        pred (np.ndarray): Predicted labels (0 or 1)\n",
    "        probas (np.ndarray): Model confidence scores for positive class\n",
    "        ref (np.ndarray): Ground truth labels\n",
    "        labels_map (dict): Mapping of label indices to class names\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing accuracy, precision, recall, f1, \n",
    "              auc, sensitivity, and specificity\n",
    "    \"\"\"\n",
    "    # Validate predictions match probability thresholds\n",
    "    assert np.array_equal(pred, (probas > 0).astype(int)), \"Predictions mismatch with probability thresholds\"\n",
    "    \n",
    "    # Calculate confusion matrix using sklearn\n",
    "    conf = confusion_matrix(ref, pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    \n",
    "    # Compute metrics using sklearn functions\n",
    "    precision = precision_score(ref, pred, zero_division=0)\n",
    "    recall = recall_score(ref, pred, zero_division=0)\n",
    "    f1 = f1_score(ref, pred, zero_division=0)\n",
    "    accuracy = accuracy_score(ref, pred)\n",
    "    specificity = tn / (tn + fp + 1e-9)\n",
    "    \n",
    "    # Calculate AUC if applicable\n",
    "    auc = roc_auc_score(ref, probas) if len(np.unique(ref)) == 2 else None\n",
    "    \n",
    "    # Print formatted results\n",
    "    '''print(f\"\\nTask: {task}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf}\")\n",
    "    print(f\"Specificity:\\t{specificity:.3f}\")\n",
    "    print(f\"Sensitivity:\\t{recall:.3f}\")  # Sensitivity = Recall\n",
    "    if auc is not None:\n",
    "        print(f\"AUC:\\t\\t{auc:.3f}\")\n",
    "    print(f\"Accuracy:\\t{accuracy:.3f}\")\n",
    "    print(f\"Precision:\\t{precision:.3f}\")\n",
    "    print(f\"F1 Score:\\t{f1:.3f}\")'''\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'sensitivity': recall,\n",
    "        'specificity': specificity\n",
    "    }\n",
    "\n",
    "def compute_stats_multiclass(pred, P, ref, labels_map):\n",
    "    # santiy check\n",
    "    assert all(map(int,P.argmax(axis=1)) == pred)\n",
    "\n",
    "    V = set(range(len(labels_map)))\n",
    "    n = max(V)+1\n",
    "    conf = np.zeros((n,n), dtype='int32')\n",
    "    for p,r in zip(pred,ref):\n",
    "        conf[p][r] += 1\n",
    "\n",
    "\n",
    "    labels = [label for label,i in sorted(labels_map.items(), key=lambda t:t[1])]\n",
    "\n",
    "\n",
    "    print(conf)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    #print('\\t prec  rec    f1   label')\n",
    "    for i in range(n):\n",
    "        label = labels[i]\n",
    "\n",
    "        tp = conf[i,i]\n",
    "        pred_pos = conf[i,:].sum()\n",
    "        ref_pos  = conf[:,i].sum()\n",
    "\n",
    "        precision   = tp / (pred_pos + 1e-9)\n",
    "        recall      = tp / (ref_pos + 1e-9)\n",
    "        f1 = (2*precision*recall) / (precision+recall+1e-9)\n",
    "\n",
    "        #print(f'\\t{precision:.3f} {recall:.3f} {f1:.3f} {label}')\n",
    "\n",
    "        # Save info\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall    = sum(recalls   ) / len(recalls   )\n",
    "    avg_f1        = sum(f1s       ) / len(f1s       )\n",
    "    #print('\\t--------------------------')\n",
    "    #print(f'\\t{avg_precision:.3f} {avg_recall:.3f} {avg_f1:.3f} avg')\n",
    "    \n",
    "    res = {'precisions':precisions, 'recalls':recalls, 'f1s':f1s}\n",
    "\n",
    "    return res\n",
    "\n",
    "def true_positive_rate(pred, ref):\n",
    "    tp,fn = 0,0\n",
    "    for p,r in zip(pred,ref):\n",
    "        if p==1 and r==1:\n",
    "            tp += 1\n",
    "        elif p==0 and r==1:\n",
    "            fn += 1\n",
    "    return tp / (tp + fn + 1e-9)\n",
    "\n",
    "\n",
    "def false_positive_rate(pred, ref):\n",
    "    fp,tn = 0,0\n",
    "    for p,r in zip(pred,ref):\n",
    "        if p==1 and r==0:\n",
    "            fp += 1\n",
    "        elif p==0 and r==0:\n",
    "            tn += 1\n",
    "    return fp / (fp + tn + 1e-9)\n",
    "\n",
    "def classification_results(model, labels_map, X, Y):\n",
    "\n",
    "    # for AUC\n",
    "    P_ = model.decision_function(X)\n",
    "\n",
    "    # sklearn has changes in API when doing binary classification. make it conform to 3+\n",
    "    if len(labels_map)==2:\n",
    "        m = X.shape[0]\n",
    "        P = np.zeros((m,2))\n",
    "        P[:,0] = -P_\n",
    "        P[:,1] =  P_\n",
    "    else:\n",
    "        P = P_\n",
    "\n",
    "    train_pred = P.argmax(axis=1)\n",
    "\n",
    "    res = compute_stats(train_pred, P, Y, labels_map)\n",
    "    return res\n",
    "    \n",
    "def regression_results(model, test_X, test_Y, description):\n",
    "    res = {}\n",
    "    \n",
    "    pred_Y = model.predict(test_X)\n",
    "    res['rms'] = mean_squared_error(test_Y, pred_Y)**0.5\n",
    "    res['mas'] = mean_absolute_error(test_Y, pred_Y)\n",
    "    '''if verbose:\n",
    "        print(description)\n",
    "        print('\\tRMS:', res['rms'])\n",
    "        print('\\tMAS:', res['mas'])\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        perfect = np.arange(min(test_Y),max(test_Y),100)\n",
    "        plt.scatter(perfect, perfect, color='red', s=0.01)\n",
    "        plt.scatter(test_Y , pred_Y, color='blue', s=1)\n",
    "        plt.xlabel('actual')\n",
    "        plt.ylabel('prediction')\n",
    "        plt.show()'''\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vec):\n",
    "    return (vec - np.mean(vec))/np.std(vec)\n",
    "\n",
    "oasis_df = pd.read_parquet(f'{DATA_LOCATION}/oasis_df.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>INSURANCE</th>\n",
       "      <th>OASIS</th>\n",
       "      <th>NONCOMPLIANT</th>\n",
       "      <th>AUTOPSY</th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE</th>\n",
       "      <th>RACE</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>LOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134462</td>\n",
       "      <td>Public</td>\n",
       "      <td>0.922227</td>\n",
       "      <td>-5.270451</td>\n",
       "      <td>-1.931002</td>\n",
       "      <td>-0.569442</td>\n",
       "      <td>M</td>\n",
       "      <td>0.412972</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>-0.554881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119940</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.020920</td>\n",
       "      <td>-6.017632</td>\n",
       "      <td>-0.203272</td>\n",
       "      <td>-0.552639</td>\n",
       "      <td>F</td>\n",
       "      <td>0.267922</td>\n",
       "      <td>White</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>-0.374724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176332</td>\n",
       "      <td>Private</td>\n",
       "      <td>-0.655060</td>\n",
       "      <td>-4.882203</td>\n",
       "      <td>-1.172165</td>\n",
       "      <td>-0.567575</td>\n",
       "      <td>M</td>\n",
       "      <td>0.358578</td>\n",
       "      <td>White</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>2.057401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108329</td>\n",
       "      <td>Public</td>\n",
       "      <td>0.922227</td>\n",
       "      <td>-5.436047</td>\n",
       "      <td>-1.139813</td>\n",
       "      <td>1.421746</td>\n",
       "      <td>F</td>\n",
       "      <td>0.594284</td>\n",
       "      <td>Black</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0.435985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>112077</td>\n",
       "      <td>Public</td>\n",
       "      <td>0.358910</td>\n",
       "      <td>-4.735420</td>\n",
       "      <td>-2.351723</td>\n",
       "      <td>-0.569975</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.040309</td>\n",
       "      <td>White</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>-0.464803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID INSURANCE     OASIS  NONCOMPLIANT   AUTOPSY  SENTIMENT GENDER  \\\n",
       "0   134462    Public  0.922227     -5.270451 -1.931002  -0.569442      M   \n",
       "1   119940   Private  0.020920     -6.017632 -0.203272  -0.552639      F   \n",
       "2   176332   Private -0.655060     -4.882203 -1.172165  -0.567575      M   \n",
       "3   108329    Public  0.922227     -5.436047 -1.139813   1.421746      F   \n",
       "4   112077    Public  0.358910     -4.735420 -2.351723  -0.569975      M   \n",
       "\n",
       "        AGE      RACE ADMISSION_TYPE       LOS  \n",
       "0  0.412972  Hispanic      EMERGENCY -0.554881  \n",
       "1  0.267922     White      EMERGENCY -0.374724  \n",
       "2  0.358578     White       ELECTIVE  2.057401  \n",
       "3  0.594284     Black      EMERGENCY  0.435985  \n",
       "4 -0.040309     White      EMERGENCY -0.464803  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes_df = pd.read_parquet(f'{DATA_LOCATION}/outcomes.parquet')\n",
    "outcomes_df['ETHNICITY'] = outcomes_df['ETHNICITY'].apply(normalize_race)\n",
    "outcomes_df['INSURANCE'] = outcomes_df['INSURANCE'].apply(normalize_insurance)\n",
    "outcomes_df = outcomes_df.rename(columns={'ETHNICITY': 'RACE', 'LOS_HOSPITAL': 'LOS'})\n",
    "for col in ['AGE', 'OASIS', 'LOS']:\n",
    "    outcomes_df[col] = normalize(outcomes_df[col])\n",
    "outcomes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(enabled):\n",
    "    demographics_features = {}\n",
    "    for i, row in outcomes_df.iterrows():\n",
    "        feats = {}\n",
    "\n",
    "        if 'admission_type' in enabled:\n",
    "            feats[('admission_type', row['ADMISSION_TYPE'])] = 1\n",
    "        if 'oasis' in enabled:\n",
    "            feats[('oasis', None)] = row['OASIS']\n",
    "\n",
    "        if 'age' in enabled:\n",
    "            feats[('age', None)] = row['AGE']\n",
    "        if 'los' in enabled:\n",
    "            feats[('los', None)] = row['LOS']\n",
    "\n",
    "        if 'insurance' in enabled:\n",
    "            feats[('insurance', row['INSURANCE'])] = 1\n",
    "        if 'gender' in enabled:\n",
    "            feats[('gender', row['GENDER'])] = 1\n",
    "\n",
    "        if 'race' in enabled:\n",
    "            feats[('race', row['RACE'])] = 1\n",
    "            \n",
    "        if 'noncompliant' in enabled:\n",
    "            feats[('noncompliant', None)] = row['NONCOMPLIANT']\n",
    "        if 'autopsy' in enabled:\n",
    "            feats[('autopsy', None)] = row['AUTOPSY']\n",
    "        if 'sentiment' in enabled:\n",
    "            feats[('sentiment', None)] = row['SENTIMENT']\n",
    "\n",
    "        demographics_features[row['HADM_ID']] = feats\n",
    "\n",
    "    # fit vectorizer\n",
    "    vect = DictVectorizer()\n",
    "    vect.fit(demographics_features.values())\n",
    "    #print('num_features:', len(vect.feature_names_))\n",
    "\n",
    "    # ordering of all features\n",
    "    ids = demographics_features.keys()\n",
    "    X = vect.transform([demographics_features[hadm_id] for hadm_id in ids])\n",
    "\n",
    "    return demographics_features, vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_string(length=6):\n",
    "    \"\"\"Generate a random string of fixed length.\"\"\"\n",
    "    random.seed()\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    return ''.join(random.choice(letters) for i in range(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_type, model_params, pred_type, target_labels_df, features, num_iterations=10):\n",
    "    all_results = defaultdict(list)\n",
    "    demographics_features, vect = build_features(features)\n",
    "    ind2feat =  {i: f for f, i in vect.vocabulary_.items()}\n",
    "    feature_weights = defaultdict(list)\n",
    "    if pred_type == 'AMA':\n",
    "        hadm_ids = list(set(discharge_df['HADM_ID'].values) & set(demographics_features.keys()))\n",
    "    elif pred_type == 'CS':\n",
    "        hadm_ids = list(set(code_labels.keys()) & set(demographics_features.keys()))\n",
    "    elif pred_type == 'MORTALITY':\n",
    "        hadm_ids = list(set(mortality_labels.keys()) & set(demographics_features.keys()))\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported prediction type: {pred_type}')\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        train_ids, test_ids = data_split(hadm_ids, seed=i)\n",
    "\n",
    "        # select pre-computed features\n",
    "        train_features = [demographics_features[hadm_id] for hadm_id in train_ids]\n",
    "        test_features  = [demographics_features[hadm_id] for hadm_id in test_ids]\n",
    "\n",
    "        # vectorize features\n",
    "        train_X = vect.transform(train_features)\n",
    "        test_X = vect.transform(test_features)\n",
    "\n",
    "        # select labels\n",
    "        train_Y = [target_labels_df.loc[hadm_id, pred_type] for hadm_id in train_ids]\n",
    "        test_Y = [target_labels_df.loc[hadm_id, pred_type] for hadm_id in test_ids]\n",
    "\n",
    "        #print('patients:', len(hadm_ids))\n",
    "        if model_type == 'lr':\n",
    "            model = LogisticRegression(**model_params)\n",
    "        elif model_type == 'gbm':\n",
    "            model = GradientBoostingClassifier(**model_params)\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported model type: {model_type}')\n",
    "\n",
    "        model.fit(train_X, train_Y)\n",
    "\n",
    "        res = classification_results(model, [0, 1], test_X, test_Y)\n",
    "        for metric, value in res.items():\n",
    "            all_results[metric].append(value)\n",
    "\n",
    "        # append coefficients to feature_weights\n",
    "        if model_type == 'lr':\n",
    "            for feat,val in enumerate(model.coef_.tolist()[0]):\n",
    "                featname = ind2feat[feat]\n",
    "                feature_weights[featname].append(val)\n",
    "        elif model_type == 'gbm':\n",
    "            for feat,val in enumerate(model.feature_importances_):\n",
    "                featname = ind2feat[feat]\n",
    "                feature_weights[featname].append(val)\n",
    "        #feature_weights.append(model)\n",
    "    all_results = {k: round(np.mean(v), 4) for k, v in all_results.items() if len(v) > 0}\n",
    "    feature_weights = {k: round(np.mean(v), 4) for k, v in feature_weights.items() if len(v) > 0}\n",
    "    return all_results, feature_weights\n",
    "\n",
    "def generate_report(params, all_results, feature_weights, filename=None):\n",
    "    # Choose output: file if filepath is given, else stdout\n",
    "    output = open(filename, 'w') if filename else sys.stdout\n",
    "    try:\n",
    "        print('Model Parameters:', file=output)\n",
    "        for param, value in params.items():\n",
    "            if isinstance(value, dict):\n",
    "                for sub_param, sub_value in value.items():\n",
    "                    print(f'\\t{sub_param}: {sub_value}', file=output)\n",
    "            else:\n",
    "                print(f'\\t{param}: {value}', file=output)\n",
    "        print('Metrics:', file=output)\n",
    "        for metric, value in all_results.items():\n",
    "            print(f'\\tAverage {metric}: {value}', file=output)\n",
    "        print('Feature Weights:', file=output)\n",
    "        for feat, weight in sorted(feature_weights.items(), key=lambda x: abs(x[1]), reverse=True):\n",
    "            feat_print = feat[0] + '=' + feat[1] if feat[1] is not None else ''\n",
    "            print(f'\\t{feat}: {weight}', file=output)\n",
    "    finally:\n",
    "        if output != sys.stdout:\n",
    "            output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed lr model for AMA prediction with features: ['age', 'los', 'insurance', 'gender']\n",
      "Completed lr model for CS prediction with features: ['age', 'los', 'insurance', 'gender']\n",
      "Completed lr model for MORTALITY prediction with features: ['age', 'los', 'insurance', 'gender']\n",
      "Completed lr model for AMA prediction with features: ['age', 'los', 'insurance', 'gender', 'race']\n",
      "Completed lr model for CS prediction with features: ['age', 'los', 'insurance', 'gender', 'race']\n",
      "Completed lr model for MORTALITY prediction with features: ['age', 'los', 'insurance', 'gender', 'race']\n",
      "Completed lr model for AMA prediction with features: ['age', 'los', 'insurance', 'gender', 'noncompliant']\n",
      "Completed lr model for CS prediction with features: ['age', 'los', 'insurance', 'gender', 'noncompliant']\n",
      "Completed lr model for MORTALITY prediction with features: ['age', 'los', 'insurance', 'gender', 'noncompliant']\n",
      "Completed lr model for AMA prediction with features: ['age', 'los', 'insurance', 'gender', 'autopsy']\n",
      "Completed lr model for CS prediction with features: ['age', 'los', 'insurance', 'gender', 'autopsy']\n",
      "Completed lr model for MORTALITY prediction with features: ['age', 'los', 'insurance', 'gender', 'autopsy']\n",
      "Completed lr model for AMA prediction with features: ['age', 'los', 'insurance', 'gender', 'sentiment']\n",
      "Completed lr model for CS prediction with features: ['age', 'los', 'insurance', 'gender', 'sentiment']\n",
      "Completed lr model for MORTALITY prediction with features: ['age', 'los', 'insurance', 'gender', 'sentiment']\n",
      "Completed lr model for AMA prediction with features: ['age', 'los', 'insurance', 'gender', 'race', 'noncompliant', 'autopsy', 'sentiment']\n",
      "Completed lr model for CS prediction with features: ['age', 'los', 'insurance', 'gender', 'race', 'noncompliant', 'autopsy', 'sentiment']\n",
      "Completed lr model for MORTALITY prediction with features: ['age', 'los', 'insurance', 'gender', 'race', 'noncompliant', 'autopsy', 'sentiment']\n",
      "Completed gbm model for AMA prediction with features: ['age', 'los', 'insurance', 'gender']\n",
      "Completed gbm model for CS prediction with features: ['age', 'los', 'insurance', 'gender']\n",
      "Completed gbm model for MORTALITY prediction with features: ['age', 'los', 'insurance', 'gender']\n",
      "Completed gbm model for AMA prediction with features: ['age', 'los', 'insurance', 'gender', 'race']\n",
      "Completed gbm model for CS prediction with features: ['age', 'los', 'insurance', 'gender', 'race']\n",
      "Completed gbm model for MORTALITY prediction with features: ['age', 'los', 'insurance', 'gender', 'race']\n",
      "Completed gbm model for AMA prediction with features: ['age', 'los', 'insurance', 'gender', 'noncompliant']\n",
      "Completed gbm model for CS prediction with features: ['age', 'los', 'insurance', 'gender', 'noncompliant']\n",
      "Completed gbm model for MORTALITY prediction with features: ['age', 'los', 'insurance', 'gender', 'noncompliant']\n",
      "Completed gbm model for AMA prediction with features: ['age', 'los', 'insurance', 'gender', 'autopsy']\n",
      "Completed gbm model for CS prediction with features: ['age', 'los', 'insurance', 'gender', 'autopsy']\n",
      "Completed gbm model for MORTALITY prediction with features: ['age', 'los', 'insurance', 'gender', 'autopsy']\n",
      "Completed gbm model for AMA prediction with features: ['age', 'los', 'insurance', 'gender', 'sentiment']\n",
      "Completed gbm model for CS prediction with features: ['age', 'los', 'insurance', 'gender', 'sentiment']\n",
      "Completed gbm model for MORTALITY prediction with features: ['age', 'los', 'insurance', 'gender', 'sentiment']\n",
      "Completed gbm model for AMA prediction with features: ['age', 'los', 'insurance', 'gender', 'race', 'noncompliant', 'autopsy', 'sentiment']\n",
      "Completed gbm model for CS prediction with features: ['age', 'los', 'insurance', 'gender', 'race', 'noncompliant', 'autopsy', 'sentiment']\n",
      "Completed gbm model for MORTALITY prediction with features: ['age', 'los', 'insurance', 'gender', 'race', 'noncompliant', 'autopsy', 'sentiment']\n",
      "All results saved to: ModelResults/model_results.csv\n"
     ]
    }
   ],
   "source": [
    "all_feature_combinations = [\n",
    "    ['age', 'los', 'insurance', 'gender'],\n",
    "    ['age', 'los', 'insurance', 'gender', 'race'],\n",
    "    ['age', 'los', 'insurance', 'gender', 'noncompliant'],\n",
    "    ['age', 'los', 'insurance', 'gender', 'autopsy'],\n",
    "    ['age', 'los', 'insurance', 'gender', 'sentiment'],\n",
    "    ['age', 'los', 'insurance', 'gender', 'race', 'noncompliant', 'autopsy', 'sentiment']\n",
    "]\n",
    "model_types = ['lr', 'gbm']\n",
    "model_params = {\n",
    "    'lr': {'C': 0.1, 'penalty': 'l2', 'tol': 0.01},\n",
    "    'gbm': {\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 3,\n",
    "        'random_state': SEED\n",
    "    }\n",
    "}\n",
    "tabled_results = pd.DataFrame(columns=['model_type', 'pred_type', 'filename', 'model_params', 'features', 'accuracy', 'precision', 'recall', 'f1', 'auc', 'sensitivity', 'specificity'])\n",
    "for model_type in model_types:\n",
    "    for features in all_feature_combinations:\n",
    "        for pred_type in ['AMA', 'CS', 'MORTALITY']:\n",
    "            results, weights = build_model(\n",
    "                model_type,\n",
    "                model_params[model_type],\n",
    "                pred_type,\n",
    "                all_labels,\n",
    "                features,\n",
    "                num_iterations=10\n",
    "            )\n",
    "            filename = f'{LOG_LOCATION}/{model_type}_{pred_type}_{random_string(6)}.txt'\n",
    "            tabled_results.loc[len(tabled_results)] = [\n",
    "                model_type,\n",
    "                pred_type,\n",
    "                filename,\n",
    "                model_params[model_type],\n",
    "                features,\n",
    "                results['accuracy'],\n",
    "                results['precision'],\n",
    "                results['recall'],\n",
    "                results['f1'],\n",
    "                results['auc'],\n",
    "                results['sensitivity'],\n",
    "                results['specificity']\n",
    "            ]\n",
    "            generate_report(\n",
    "                {'model_type': model_type, 'pred_type': pred_type, 'use_features': features, 'model_params': model_params[model_type]},\n",
    "                results,\n",
    "                weights,\n",
    "                filename=filename\n",
    "            )\n",
    "            print(f'Completed {model_type} model for {pred_type} prediction with features: {features}')\n",
    "tabled_results.to_csv(f'{LOG_LOCATION}/model_results.csv', index=False)\n",
    "print('All results saved to:', f'{LOG_LOCATION}/model_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
